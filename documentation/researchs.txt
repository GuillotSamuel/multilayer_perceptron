### BASICS ###


# FUNCTIONS #

1. Fonctions d'activation
Sigmoid : Utile pour les couches de sortie dans les probl√®mes de classification binaire, mais elle peut causer des probl√®mes de "vanishing gradients" dans les r√©seaux profonds.

ReLU (Rectified Linear Unit) : Tr√®s populaire pour les couches cach√©es car elle est simple et aide √† att√©nuer le probl√®me des gradients disparaissants. Elle est d√©finie par 
ReLU
(
x
)
=
max
‚Å°
(
0
,
x
)
ReLU(x)=max(0,x).

Softmax : Utilis√©e pour la couche de sortie dans les probl√®mes de classification multi-classes. Elle transforme les scores en probabilit√©s.

Tanh : Une alternative √† la sigmo√Øde, avec une sortie comprise entre -1 et 1. Elle peut aussi souffrir de "vanishing gradients".

2. Fonctions de perte (Loss functions)
Mean Squared Error (MSE) : Utilis√©e pour les probl√®mes de r√©gression. Elle mesure la moyenne des carr√©s des diff√©rences entre les valeurs pr√©dites et r√©elles.

Cross-Entropy Loss : Id√©ale pour les probl√®mes de classification. Elle mesure la performance d'un mod√®le de classification dont la sortie est une probabilit√© entre 0 et 1.

Binary Cross-Entropy : Pour les probl√®mes de classification binaire.

Categorical Cross-Entropy : Pour les probl√®mes de classification multi-classes.

3. Optimisation
Descente de gradient : La m√©thode de base pour minimiser la fonction de perte.

Stochastic Gradient Descent (SGD) : Une variante qui utilise un sous-ensemble al√©atoire des donn√©es √† chaque √©tape, ce qui peut acc√©l√©rer l'apprentissage.

Adam : Un optimiseur adaptatif qui combine les avantages de RMSprop et de la descente de gradient avec momentum. Il est souvent plus efficace et plus rapide √† converger.

4. D√©tection d'entr√©es hors distribution (OOD)
Distance de Mahalanobis : Une m√©thode pour mesurer la distance d'un point par rapport √† une distribution. Elle peut √™tre utilis√©e pour d√©tecter des entr√©es qui ne ressemblent pas aux donn√©es d'entra√Ænement.

Confiance du Softmax : La probabilit√© maximale de sortie du softmax peut √™tre utilis√©e comme indicateur de confiance. Une faible confiance peut indiquer une entr√©e hors distribution.

Autoencodeurs : Ils peuvent √™tre utilis√©s pour apprendre une repr√©sentation compacte des donn√©es d'entra√Ænement. Les entr√©es qui ne peuvent pas √™tre bien reconstruites peuvent √™tre consid√©r√©es comme hors distribution.

5. Ce qu'on attend d'un MLP
Capacit√© d'apprentissage : Un MLP doit √™tre capable d'apprendre des relations complexes et non lin√©aires dans les donn√©es.

G√©n√©ralisation : Il doit bien performer sur des donn√©es jamais vues auparavant, pas seulement sur les donn√©es d'entra√Ænement.

Robustesse : Il doit √™tre capable de g√©rer du bruit et des variations dans les donn√©es.

Interpr√©tabilit√© : Dans certains cas, il est important de comprendre pourquoi le mod√®le a pris une certaine d√©cision, bien que cela soit plus difficile avec les MLP qu'avec des mod√®les plus simples.

6. Bonnes pratiques
Normalisation des donn√©es : Assure-toi que tes donn√©es sont normalis√©es ou standardis√©es pour am√©liorer la convergence.

Initialisation des poids : Utilise des m√©thodes comme Xavier ou He initialization pour √©viter les probl√®mes de gradients disparaissants ou explosifs.

R√©gularisation : Techniques comme L2 regularization (weight decay) ou dropout pour √©viter le surapprentissage.

Validation crois√©e : Utilise-la pour √©valuer la performance de ton mod√®le et ajuster les hyperparam√®tres.


# STRUCTURE #

Pour organiser les donn√©es d'entr√©e de ton multilayer perceptron (MLP), voici une arborescence simple et efficace que tu peux suivre :

Copy
projet_mlp/
‚îÇ
‚îú‚îÄ‚îÄ data/                     # Dossier contenant toutes les donn√©es
‚îÇ   ‚îú‚îÄ‚îÄ raw/                  # Donn√©es brutes (non trait√©es)
‚îÇ   ‚îú‚îÄ‚îÄ processed/            # Donn√©es pr√©trait√©es (normalis√©es, nettoy√©es, etc.)
‚îÇ   ‚îî‚îÄ‚îÄ splits/               # Donn√©es divis√©es en ensembles d'entra√Ænement, validation et test
‚îÇ       ‚îú‚îÄ‚îÄ train/            # Donn√©es d'entra√Ænement
‚îÇ       ‚îú‚îÄ‚îÄ val/              # Donn√©es de validation
‚îÇ       ‚îî‚îÄ‚îÄ test/             # Donn√©es de test
‚îÇ
‚îú‚îÄ‚îÄ models/                   # Dossier pour sauvegarder les mod√®les entra√Æn√©s
‚îÇ
‚îú‚îÄ‚îÄ scripts/                  # Scripts pour le pr√©traitement, l'entra√Ænement, etc.
‚îÇ   ‚îú‚îÄ‚îÄ preprocess.py         # Script de pr√©traitement des donn√©es
‚îÇ   ‚îú‚îÄ‚îÄ train.py              # Script d'entra√Ænement du mod√®le
‚îÇ   ‚îî‚îÄ‚îÄ evaluate.py           # Script d'√©valuation du mod√®le
‚îÇ
‚îî‚îÄ‚îÄ README.md                 # Documentation du projet
Explications :
data/raw/ : Stocke ici les donn√©es brutes que tu r√©cup√®res (fichiers CSV, images, etc.).

data/processed/ : Apr√®s pr√©traitement (nettoyage, normalisation, etc.), sauvegarde les donn√©es trait√©es ici.

data/splits/ : Divise tes donn√©es en 3 ensembles :

train/ : Pour l'entra√Ænement du mod√®le.

val/ : Pour ajuster les hyperparam√®tres et √©viter le surapprentissage.

test/ : Pour √©valuer la performance finale du mod√®le.

models/ : Sauvegarde les mod√®les entra√Æn√©s ici (poids, architecture, etc.).

scripts/ : Organise ton code en scripts s√©par√©s pour le pr√©traitement, l'entra√Ænement et l'√©valuation.

Pourquoi cette structure ?
Clart√© : Tout est bien organis√© et facile √† retrouver.

Reproductibilit√© : Les donn√©es brutes et trait√©es sont s√©par√©es, ce qui permet de reproduire les √©tapes facilement.

Modularit√© : Les scripts sont s√©par√©s pour faciliter la maintenance et les modifications.

C'est une structure classique et efficace pour la majorit√© des projets de machine learning. üòä



### ADVANCED ###


# FORWARD / BACKWARD PROPAGATION #

Je vais vous expliquer la propagation avant (forward propagation) et la r√©tropropagation (back propagation) dans les r√©seaux de neurones.
La Propagation Avant (Forward Propagation):
C'est le processus par lequel l'information traverse le r√©seau de neurones de l'entr√©e vers la sortie. Voici les √©tapes :

Les donn√©es d'entr√©e (X) sont introduites dans la premi√®re couche
Pour chaque couche, on calcule :

La somme pond√©r√©e des entr√©es (Z = W√óX + b)
L'application d'une fonction d'activation (A = f(Z))


Ce processus se r√©p√®te jusqu'√† la couche de sortie
On obtient une pr√©diction (≈∑)

pythonCopydef forward_propagation(X, W1, b1, W2, b2):
    # Premi√®re couche
    Z1 = np.dot(W1, X) + b1
    A1 = np.tanh(Z1)
    
    # Deuxi√®me couche
    Z2 = np.dot(W2, A1) + b2
    A2 = sigmoid(Z2)
    
    return Z1, A1, Z2, A2
La R√©tropropagation (Back Propagation):
C'est l'algorithme qui permet d'ajuster les poids du r√©seau pour minimiser l'erreur. Il fonctionne en "remontant" le r√©seau de la sortie vers l'entr√©e :

On calcule d'abord l'erreur √† la sortie (diff√©rence entre pr√©diction et valeur r√©elle)
Pour chaque couche, de la fin vers le d√©but :

On calcule le gradient de l'erreur par rapport aux poids
On calcule le gradient par rapport aux activations
On propage l'erreur vers la couche pr√©c√©dente


On met √† jour les poids avec ces gradients

pythonCopydef backward_propagation(X, Y, A1, A2, W2, Z1, Z2):
    m = Y.shape[1]
    
    # Gradients couche de sortie
    dZ2 = A2 - Y
    dW2 = (1/m) * np.dot(dZ2, A1.T)
    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)
    
    # Gradients premi√®re couche
    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))
    dW1 = (1/m) * np.dot(dZ1, X.T)
    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)
    
    return dW1, db1, dW2, db2
Les points cl√©s √† retenir :

La propagation avant calcule la pr√©diction
La r√©tropropagation calcule les gradients
Les gradients indiquent comment modifier les poids pour r√©duire l'erreur
Ce processus est r√©p√©t√© de nombreuses fois (√©poques) pour entra√Æner le r√©seau


# GRADIENTS TYPES #

Je vais vous expliquer les diff√©rents types de gradients qu'on rencontre dans les r√©seaux de neurones :

Gradient par rapport aux poids (dW) :

Il indique comment l'erreur change quand on modifie les poids
Notation : ‚àÇL/‚àÇW (d√©riv√©e partielle de la fonction de perte L par rapport aux poids W)
Utilis√© pour mettre √† jour les poids : W = W - Œ±√ódW (Œ± √©tant le taux d'apprentissage)

Gradient par rapport aux biais (db) :

Montre l'impact du changement des biais sur l'erreur
Notation : ‚àÇL/‚àÇb
Mis √† jour de fa√ßon similaire aux poids : b = b - Œ±√ódb

Gradient par rapport aux activations (dA) :

Mesure comment l'erreur change par rapport aux sorties des fonctions d'activation
Notation : ‚àÇL/‚àÇA
Important pour propager l'erreur entre les couches

Gradient par rapport aux valeurs pr√©-activation (dZ) :

Calcule la variation de l'erreur par rapport aux valeurs avant la fonction d'activation
Notation : ‚àÇL/‚àÇZ
dZ = dA * f'(Z) o√π f' est la d√©riv√©e de la fonction d'activation

pythonCopy# Exemple de calcul des diff√©rents gradients
def compute_gradients(dZ, A_prev, W, b, m):
    # Gradient par rapport aux poids
    dW = (1/m) * np.dot(dZ, A_prev.T)
    
    # Gradient par rapport aux biais
    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)
    
    # Gradient pour la couche pr√©c√©dente
    dA_prev = np.dot(W.T, dZ)
    
    return dW, db, dA_prev

Gradient du co√ªt total (dJ) :

Repr√©sente la direction de la plus forte augmentation de la fonction de co√ªt
Combine tous les gradients partiels
Utilis√© pour √©valuer la convergence de l'apprentissage

Gradient vanishing (√©vanouissement du gradient) :

Probl√®me o√π les gradients deviennent tr√®s petits dans les couches profondes
Solutions :

Fonctions d'activation comme ReLU
Connections r√©siduelles (Skip connections)
Normalisation par lots (Batch normalization)


Gradient exploding (explosion du gradient) :


Probl√®me inverse o√π les gradients deviennent tr√®s grands
Solutions :

Gradient clipping
Normalisation des poids
Architectures adapt√©es

# BINARY / MULTIPLE ANSWER #

import numpy as np

class FlexibleMLP:
    def __init__(self, input_size, hidden_sizes, output_size):
        self.layers_dims = [input_size] + hidden_sizes + [output_size]
        self.parameters = self._initialize_parameters()
        
    def _initialize_parameters(self):
        parameters = {}
        for l in range(1, len(self.layers_dims)):
            parameters[f'W{l}'] = np.random.randn(self.layers_dims[l], self.layers_dims[l-1]) * 0.01
            parameters[f'b{l}'] = np.zeros((self.layers_dims[l], 1))
        return parameters
    
    def _forward_propagation(self, X):
        cache = {'A0': X}
        A = X
        L = len(self.layers_dims) - 1  # nombre de couches
        
        # Propagation √† travers les couches cach√©es (ReLU)
        for l in range(1, L):
            Z = np.dot(self.parameters[f'W{l}'], A) + self.parameters[f'b{l}']
            A = np.maximum(0, Z)  # ReLU
            cache[f'Z{l}'] = Z
            cache[f'A{l}'] = A
        
        # Couche de sortie (Sigmoid pour binaire, Softmax pour multiple)
        ZL = np.dot(self.parameters[f'W{L}'], A) + self.parameters[f'b{L}']
        if self.layers_dims[-1] == 1:  # cas binaire
            AL = 1 / (1 + np.exp(-ZL))  # sigmoid
        else:  # cas multiple
            exp_scores = np.exp(ZL - np.max(ZL, axis=0, keepdims=True))
            AL = exp_scores / np.sum(exp_scores, axis=0, keepdims=True)  # softmax
            
        cache[f'Z{L}'] = ZL
        cache[f'A{L}'] = AL
        
        return AL, cache
    
    def _backward_propagation(self, AL, Y, cache):
        grads = {}
        L = len(self.layers_dims) - 1
        m = Y.shape[1]
        
        # Initialisation de la d√©riv√©e de la perte
        if self.layers_dims[-1] == 1:  # cas binaire
            dZL = AL - Y  # d√©riv√©e de la BCE (Binary Cross Entropy)
        else:  # cas multiple
            dZL = AL - Y  # d√©riv√©e de la CCE (Categorical Cross Entropy)
        
        # Gradient pour la derni√®re couche
        grads[f'dW{L}'] = (1/m) * np.dot(dZL, cache[f'A{L-1}'].T)
        grads[f'db{L}'] = (1/m) * np.sum(dZL, axis=1, keepdims=True)
        
        # R√©tropropagation √† travers les couches cach√©es
        for l in range(L-1, 0, -1):
            dA = np.dot(self.parameters[f'W{l+1}'].T, dZL)
            dZ = dA * (cache[f'A{l}'] > 0)  # d√©riv√©e de ReLU
            
            grads[f'dW{l}'] = (1/m) * np.dot(dZ, cache[f'A{l-1}'].T)
            grads[f'db{l}'] = (1/m) * np.sum(dZ, axis=1, keepdims=True)
            dZL = dZ
        
        return grads
    
    def _update_parameters(self, grads, learning_rate):
        L = len(self.layers_dims) - 1
        for l in range(1, L + 1):
            self.parameters[f'W{l}'] -= learning_rate * grads[f'dW{l}']
            self.parameters[f'b{l}'] -= learning_rate * grads[f'db{l}']
    
    def train(self, X, Y, learning_rate=0.01, epochs=1000):
        costs = []
        
        for i in range(epochs):
            # Forward propagation
            AL, cache = self._forward_propagation(X)
            
            # Calcul du co√ªt
            if self.layers_dims[-1] == 1:  # cas binaire
                cost = -np.mean(Y * np.log(AL + 1e-8) + (1-Y) * np.log(1-AL + 1e-8))
            else:  # cas multiple
                cost = -np.mean(np.sum(Y * np.log(AL + 1e-8), axis=0))
            
            # Backward propagation
            grads = self._backward_propagation(AL, Y, cache)
            
            # Mise √† jour des param√®tres
            self._update_parameters(grads, learning_rate)
            
            if i % 100 == 0:
                costs.append(cost)
                
        return costs
    
    def predict(self, X):
        AL, _ = self._forward_propagation(X)
        if self.layers_dims[-1] == 1:  # cas binaire
            return (AL > 0.5).astype(int)
        else:  # cas multiple
            return np.argmax(AL, axis=0)

Cette impl√©mentation offre plusieurs avantages cl√©s :

Flexibilit√© de la sortie :

D√©tecte automatiquement si c'est un cas binaire (output_size=1) ou multiple (output_size>1)
Utilise sigmoid pour le binaire et softmax pour le multiclasse
Adapte la fonction de co√ªt automatiquement (BCE ou CCE)

Architecture modulaire :

Initialisation adaptative selon les dimensions sp√©cifi√©es
Forward propagation avec ReLU dans les couches cach√©es
Backward propagation qui g√®re les deux cas

Exemple d'utilisation :
pythonCopy# Pour classification binaire
mlp_binary = FlexibleMLP(input_size=4, hidden_sizes=[8, 4], output_size=1)

# Pour classification multiple
mlp_multi = FlexibleMLP(input_size=4, hidden_sizes=[8, 4], output_size=3)

# Entra√Ænement
costs = mlp_binary.train(X_train, y_train, learning_rate=0.01, epochs=1000)

# ACTIVATION FUNCTIONS CLASS #

import numpy as np

class ActivationFunctions:
    @staticmethod
    def sigmoid(x, derivative=False):
        """
        Sigmoid activation function: f(x) = 1 / (1 + e^(-x))
        """
        sigmoid_x = 1 / (1 + np.exp(-x))
        if derivative:
            return sigmoid_x * (1 - sigmoid_x)
        return sigmoid_x

    @staticmethod
    def tanh(x, derivative=False):
        """
        Hyperbolic tangent activation function: f(x) = (e^x - e^(-x)) / (e^x + e^(-x))
        """
        if derivative:
            return 1 - np.tanh(x) ** 2
        return np.tanh(x)

    @staticmethod
    def relu(x, derivative=False):
        """
        Rectified Linear Unit (ReLU): f(x) = max(0, x)
        """
        if derivative:
            return np.where(x > 0, 1, 0)
        return np.maximum(0, x)

    @staticmethod
    def leaky_relu(x, alpha=0.01, derivative=False):
        """
        Leaky ReLU: f(x) = max(Œ±x, x) where Œ± is a small positive constant
        """
        if derivative:
            return np.where(x > 0, 1, alpha)
        return np.where(x > 0, x, x * alpha)

    @staticmethod
    def elu(x, alpha=1.0, derivative=False):
        """
        Exponential Linear Unit (ELU): f(x) = x if x > 0 else Œ±(e^x - 1)
        """
        if derivative:
            return np.where(x > 0, 1, alpha * np.exp(x))
        return np.where(x > 0, x, alpha * (np.exp(x) - 1))

    @staticmethod
    def softmax(x, derivative=False):
        """
        Softmax activation function: f(x_i) = e^(x_i) / Œ£(e^(x_j))
        Note: derivative not implemented as it requires Jacobian matrix
        """
        if derivative:
            raise NotImplementedError("Softmax derivative requires Jacobian matrix calculation")
        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)

    @staticmethod
    def swish(x, beta=1.0, derivative=False):
        """
        Swish activation function: f(x) = x * sigmoid(Œ≤x)
        """
        sigmoid_bx = 1 / (1 + np.exp(-beta * x))
        if derivative:
            return beta * sigmoid_bx + x * beta * sigmoid_bx * (1 - sigmoid_bx)
        return x * sigmoid_bx

    @staticmethod
    def gelu(x, derivative=False):
        """
        Gaussian Error Linear Unit (GELU): f(x) = x * Œ¶(x)
        where Œ¶(x) is the cumulative distribution function of the standard normal distribution
        """
        if derivative:
            cdf = 0.5 * (1 + np.tanh(np.sqrt(2/np.pi) * (x + 0.044715 * x**3)))
            pdf = np.exp(-0.5 * x**2) / np.sqrt(2*np.pi)
            return cdf + x * pdf
        return 0.5 * x * (1 + np.tanh(np.sqrt(2/np.pi) * (x + 0.044715 * x**3)))

    @staticmethod
    def selu(x, derivative=False):
        """
        Scaled Exponential Linear Unit (SELU)
        """
        alpha = 1.6732632423543772848170429916717
        scale = 1.0507009873554804934193349852946
        
        if derivative:
            return scale * np.where(x > 0, 1, alpha * np.exp(x))
        return scale * np.where(x > 0, x, alpha * (np.exp(x) - 1))

    @staticmethod
    def mish(x, derivative=False):
        """
        Mish activation function: f(x) = x * tanh(softplus(x))
        where softplus(x) = ln(1 + e^x)
        """
        softplus = np.log1p(np.exp(x))
        tanh_softplus = np.tanh(softplus)
        
        if derivative:
            sech_squared = 1 - tanh_softplus ** 2
            sigmoid = 1 / (1 + np.exp(-x))
            return tanh_softplus + x * sech_squared * sigmoid
        return x * tanh_softplus

Caract√©ristiques principales :

Toutes les fonctions sont impl√©ment√©es comme m√©thodes statiques
Chaque fonction a un param√®tre derivative pour calculer la d√©riv√©e
Utilisation de numpy pour des calculs vectoris√©s efficaces
Documentation pour chaque fonction avec sa formule math√©matique

Les fonctions incluses sont :

Sigmoid (pour la classification binaire)
Tanh (alternative √† sigmoid, sortie centr√©e autour de 0)
ReLU (la plus utilis√©e dans les couches cach√©es)
Leaky ReLU (pour √©viter le probl√®me du "dying ReLU")
ELU (version lisse de ReLU)
Softmax (pour la classification multiclasse)
Swish (fonction d'activation moderne de Google)
GELU (utilis√©e dans les transformers)
SELU (auto-normalisante)
Mish (alternative moderne √† ReLU)


# OTHER TOOLS CLASS #

import numpy as np
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt

class MLPUtils:
    @staticmethod
    def initialize_weights(layer_dims, initialization="he"):
        """
        Initialize weights for neural network with different methods
        """
        parameters = {}
        L = len(layer_dims)

        for l in range(1, L):
            if initialization == "he":
                # He initialization for ReLU
                parameters[f'W{l}'] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2./layer_dims[l-1])
            elif initialization == "xavier":
                # Xavier initialization for tanh
                parameters[f'W{l}'] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(1./layer_dims[l-1])
            elif initialization == "zero":
                parameters[f'W{l}'] = np.zeros((layer_dims[l], layer_dims[l-1]))
            else:  # small random
                parameters[f'W{l}'] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01
            
            parameters[f'b{l}'] = np.zeros((layer_dims[l], 1))

        return parameters

    @staticmethod
    def one_hot_encode(y, num_classes=None):
        """
        Convert class labels to one-hot encoded format
        """
        if num_classes is None:
            num_classes = len(np.unique(y))
        return np.eye(num_classes)[y.reshape(-1)]

    @staticmethod
    def normalize_data(X, method="minmax"):
        """
        Normalize input data using different methods
        """
        if method == "minmax":
            return (X - X.min()) / (X.max() - X.min())
        elif method == "zscore":
            return (X - X.mean()) / X.std()
        elif method == "l2":
            return X / np.sqrt(np.sum(X**2, axis=0))
        return X

    @staticmethod
    def compute_cost(AL, Y, cost_function="binary_crossentropy"):
        """
        Compute the cost using different loss functions
        """
        m = Y.shape[1]
        epsilon = 1e-15  # Pour √©viter log(0)
        
        if cost_function == "binary_crossentropy":
            cost = -np.mean(Y * np.log(AL + epsilon) + (1-Y) * np.log(1-AL + epsilon))
        elif cost_function == "categorical_crossentropy":
            cost = -np.mean(np.sum(Y * np.log(AL + epsilon), axis=0))
        elif cost_function == "mse":
            cost = np.mean(np.square(AL - Y))
        else:
            raise ValueError("Cost function not supported")
            
        return cost

    @staticmethod
    def plot_training_history(costs, title="Training History"):
        """
        Plot the training cost history
        """
        plt.figure(figsize=(10, 6))
        plt.plot(costs)
        plt.title(title)
        plt.xlabel('Iterations (x100)')
        plt.ylabel('Cost')
        plt.grid(True)
        plt.show()

    @staticmethod
    def compute_metrics(y_true, y_pred, binary=True):
        """
        Compute various classification metrics
        """
        conf_matrix = confusion_matrix(y_true, y_pred)
        
        if binary:
            tn, fp, fn, tp = conf_matrix.ravel()
            accuracy = (tp + tn) / (tp + tn + fp + fn)
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
            
            return {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'confusion_matrix': conf_matrix
            }
        else:
            return {
                'classification_report': classification_report(y_true, y_pred),
                'confusion_matrix': conf_matrix
            }

    @staticmethod
    def add_regularization(cost, parameters, lambd, m):
        """
        Add L2 regularization to the cost
        """
        L2_cost = 0
        for key in parameters:
            if key.startswith('W'):
                L2_cost += np.sum(np.square(parameters[key]))
        L2_cost = (lambd / (2 * m)) * L2_cost
        return cost + L2_cost

    @staticmethod
    def early_stopping_check(val_costs, patience=5, min_delta=1e-4):
        """
        Check if training should stop based on validation cost
        """
        if len(val_costs) < patience + 1:
            return False
            
        for i in range(patience):
            if val_costs[-i-2] - val_costs[-i-1] > min_delta:
                return False
        return True

    @staticmethod
    def mini_batch_generator(X, Y, batch_size=32, shuffle=True):
        """
        Generate mini-batches from training data
        """
        m = X.shape[1]
        mini_batches = []
        
        if shuffle:
            permutation = np.random.permutation(m)
            X = X[:, permutation]
            Y = Y[:, permutation]
        
        num_complete_batches = m // batch_size
        
        for k in range(num_complete_batches):
            mini_batch_X = X[:, k * batch_size:(k + 1) * batch_size]
            mini_batch_Y = Y[:, k * batch_size:(k + 1) * batch_size]
            mini_batches.append((mini_batch_X, mini_batch_Y))
        
        if m % batch_size != 0:
            mini_batch_X = X[:, num_complete_batches * batch_size:]
            mini_batch_Y = Y[:, num_complete_batches * batch_size:]
            mini_batches.append((mini_batch_X, mini_batch_Y))
            
        return mini_batches

Cette classe utilitaire fournit plusieurs outils essentiels :

Initialisation des poids :

He (pour ReLU)
Xavier (pour tanh)
Initialisation √† z√©ro ou al√©atoire

Pr√©traitement des donn√©es :

One-hot encoding pour les labels
Normalisation (min-max, z-score, L2)


Fonctions de co√ªt :

Binary cross-entropy
Categorical cross-entropy
MSE

Outils de monitoring :

Affichage de l'historique d'entra√Ænement
Calcul des m√©triques (accuracy, precision, recall, F1)
Matrice de confusion

Techniques d'optimisation :

R√©gularisation L2
Early stopping
Mini-batch generator

Exemple d'utilisation :
pythonCopy# Cr√©er une instance
utils = MLPUtils()

# Pr√©traitement
X_norm = utils.normalize_data(X_train, method="zscore")
y_encoded = utils.one_hot_encode(y_train)

# Initialisation des poids
layers = [input_size, 64, 32, output_size]
parameters = utils.initialize_weights(layers, initialization="he")

# G√©n√©ration de mini-batches
mini_batches = utils.mini_batch_generator(X_norm, y_encoded, batch_size=32)

# Calcul des m√©triques
metrics = utils.compute_metrics(y_true, y_pred, binary=True)


# AUTO MODEL CONFIG #

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import optuna
from typing import List, Tuple, Dict, Union

class AutoMLP:
    def __init__(self, X, y, verbose=True):
        self.X = X
        self.y = y
        self.verbose = verbose
        self.input_size = X.shape[1]
        self.output_size = len(np.unique(y)) if len(y.shape) == 1 else y.shape[1]
        self.is_binary = self.output_size == 2
        self.best_params = None
        self.best_model = None
        
    def _analyze_data_complexity(self) -> Dict:
        """
        Analyse la complexit√© des donn√©es pour sugg√©rer une architecture initiale
        """
        n_samples = self.X.shape[0]
        n_features = self.X.shape[1]
        
        # Calculer la variance expliqu√©e pour estimer la complexit√©
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(self.X)
        eigenvalues = np.linalg.eigvals(np.cov(X_scaled.T))
        explained_variance_ratio = eigenvalues / np.sum(eigenvalues)
        
        # Estimer la complexit√© bas√©e sur plusieurs facteurs
        complexity_score = {
            'data_size': min(1.0, n_samples / 10000),  # Normaliser par rapport √† un grand dataset
            'feature_complexity': np.sum(explained_variance_ratio > 0.01),  # Nombre de composantes significatives
            'feature_ratio': n_features / n_samples,
            'class_imbalance': np.std(np.bincount(self.y if len(self.y.shape) == 1 else np.argmax(self.y, axis=1)))
        }
        
        return complexity_score
    
    def _suggest_initial_architecture(self, complexity: Dict) -> Dict:
        """
        Sugg√®re une architecture initiale bas√©e sur l'analyse de complexit√©
        """
        # Calculer le nombre de couches sugg√©r√©
        suggested_layers = max(2, int(complexity['feature_complexity'] * 1.5))
        
        # Calculer la taille des couches
        initial_layer_size = min(2048, max(64, self.input_size * 2))
        decay_factor = np.exp(np.log(self.output_size / initial_layer_size) / suggested_layers)
        
        layer_sizes = [int(initial_layer_size * (decay_factor ** i)) for i in range(suggested_layers)]
        
        return {
            'n_layers': suggested_layers,
            'layer_sizes': layer_sizes,
            'suggested_batch_size': min(64, max(16, int(np.sqrt(self.X.shape[0])))),
            'suggested_learning_rate': 0.001 if complexity['data_size'] > 0.5 else 0.01
        }
    
    def _objective(self, trial) -> float:
        """
        Fonction objective pour Optuna (optimisation des hyperparam√®tres)
        """
        # Sugg√©rer des hyperparam√®tres
        n_layers = trial.suggest_int('n_layers', 2, 5)
        
        # Cr√©er l'architecture des couches
        layer_sizes = []
        for i in range(n_layers):
            layer_sizes.append(trial.suggest_int(f'layer_{i}', 32, 512))
            
        # Autres hyperparam√®tres
        learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)
        batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])
        dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)
        activation = trial.suggest_categorical('activation', ['relu', 'elu', 'tanh'])
        
        # Cr√©er et entra√Æner le mod√®le
        model = self._create_model(layer_sizes, activation, dropout_rate)
        history = self._train_model(model, learning_rate, batch_size)
        
        return min(history['val_loss'])
    
    def optimize_architecture(self, n_trials=100) -> None:
        """
        Optimise l'architecture du mod√®le
        """
        if self.verbose:
            print("Analyzing data complexity...")
        
        complexity = self._analyze_data_complexity()
        initial_arch = self._suggest_initial_architecture(complexity)
        
        if self.verbose:
            print("Starting hyperparameter optimization...")
        
        study = optuna.create_study(direction='minimize')
        study.optimize(self._objective, n_trials=n_trials)
        
        self.best_params = study.best_params
        self.best_model = self._create_model(
            [self.best_params[f'layer_{i}'] for i in range(self.best_params['n_layers'])],
            self.best_params['activation'],
            self.best_params['dropout_rate']
        )
    
    def _create_model(self, layer_sizes: List[int], activation: str, dropout_rate: float):
        """
        Cr√©e le mod√®le avec les param√®tres sp√©cifi√©s
        """
        from tensorflow.keras.models import Sequential
        from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
        
        model = Sequential()
        
        # Couche d'entr√©e
        model.add(Dense(layer_sizes[0], input_dim=self.input_size, activation=activation))
        model.add(BatchNormalization())
        model.add(Dropout(dropout_rate))
        
        # Couches cach√©es
        for size in layer_sizes[1:]:
            model.add(Dense(size, activation=activation))
            model.add(BatchNormalization())
            model.add(Dropout(dropout_rate))
        
        # Couche de sortie
        if self.is_binary:
            model.add(Dense(1, activation='sigmoid'))
        else:
            model.add(Dense(self.output_size, activation='softmax'))
            
        return model
    
    def _train_model(self, model, learning_rate: float, batch_size: int) -> Dict:
        """
        Entra√Æne le mod√®le avec les param√®tres sp√©cifi√©s
        """
        from tensorflow.keras.optimizers import Adam
        from tensorflow.keras.callbacks import EarlyStopping
        
        # Compiler le mod√®le
        if self.is_binary:
            loss = 'binary_crossentropy'
        else:
            loss = 'categorical_crossentropy'
            
        model.compile(
            optimizer=Adam(learning_rate=learning_rate),
            loss=loss,
            metrics=['accuracy']
        )
        
        # Early stopping
        early_stopping = EarlyStopping(
            monitor='val_loss',
            patience=10,
            restore_best_weights=True
        )
        
        # Entra√Æner le mod√®le
        X_train, X_val, y_train, y_val = train_test_split(
            self.X, self.y, test_size=0.2, random_state=42
        )
        
        history = model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=100,
            batch_size=batch_size,
            callbacks=[early_stopping],
            verbose=0
        )
        
        return history.history
    
    def get_best_model(self):
        """
        Retourne le meilleur mod√®le trouv√©
        """
        if self.best_model is None:
            raise ValueError("Model not optimized yet. Call optimize_architecture() first.")
        return self.best_model

Cette impl√©mentation offre plusieurs fonctionnalit√©s cl√©s :

Analyse automatique de la complexit√© des donn√©es :


Taille du dataset
Complexit√© des features
Ratio features/√©chantillons
D√©s√©quilibre des classes


Suggestion d'architecture initiale bas√©e sur :


Nombre de couches optimal
Taille des couches
Batch size
Learning rate


Optimisation automatique via Optuna :


Nombre de couches
Taille des couches
Taux d'apprentissage
Taille des batches
Taux de dropout
Fonction d'activation

Exemple d'utilisation :
pythonCopy# Cr√©er l'instance
auto_mlp = AutoMLP(X, y, verbose=True)

# Optimiser l'architecture
auto_mlp.optimize_architecture(n_trials=100)

# R√©cup√©rer le meilleur mod√®le
best_model = auto_mlp.get_best_model()

# Voir les meilleurs param√®tres
print(auto_mlp.best_params)
Avantages de cette approche :

Automatisation compl√®te de la conception du r√©seau
Adaptation √† la complexit√© des donn√©es
Optimisation des hyperparam√®tres
Gestion automatique des cas binaires et multiclasses


# HORS DISTRIBUTION PROBLEM MANAGEMENT #

import numpy as np
from sklearn.covariance import EmpiricalCovariance
from scipy.stats import chi2

class RobustMLP:
    def __init__(self, input_size, hidden_sizes, output_size, confidence_threshold=0.95):
        self.mlp = FlexibleMLP(input_size, hidden_sizes, output_size)
        self.confidence_threshold = confidence_threshold
        self.feature_covariance = None
        self.feature_mean = None
        self.softmax_threshold = 0.7  # Seuil minimal pour la confiance de classification
        self.mahalanobis_threshold = None
        
    def fit(self, X, y):
        """
        Entra√Æne le mod√®le et calcule les statistiques de distribution
        """
        # Entra√Æner le MLP standard
        self.mlp.train(X, y)
        
        # Calculer les statistiques de distribution des features
        self.feature_mean = np.mean(X, axis=0)
        self.feature_covariance = EmpiricalCovariance().fit(X)
        
        # Calculer le seuil de Mahalanobis bas√© sur la distribution chi2
        n_features = X.shape[1]
        self.mahalanobis_threshold = chi2.ppf(self.confidence_threshold, df=n_features)
        
        # Collecter les statistiques d'activation de la derni√®re couche cach√©e
        self._collect_activation_stats(X)
        
    def _collect_activation_stats(self, X):
        """
        Collecte les statistiques d'activation pour chaque classe
        """
        # Obtenir les activations de la derni√®re couche cach√©e
        activations = self._get_hidden_activations(X)
        self.activation_mean = np.mean(activations, axis=0)
        self.activation_cov = np.cov(activations.T)
        
    def _get_hidden_activations(self, X):
        """
        Obtient les activations de la derni√®re couche cach√©e
        """
        # Cette m√©thode d√©pend de l'impl√©mentation sp√©cifique de votre MLP
        # Ici on suppose qu'elle retourne les activations de la derni√®re couche cach√©e
        return self.mlp._forward_propagation(X)[1][f'A{len(self.mlp.layers_dims)-2}'].T
        
    def _compute_mahalanobis_distance(self, x):
        """
        Calcule la distance de Mahalanobis pour d√©tecter les outliers
        """
        x_centered = x - self.feature_mean
        inv_covariance = self.feature_covariance.get_precision()
        return np.sqrt(np.dot(np.dot(x_centered, inv_covariance), x_centered.T))
        
    def predict_with_confidence(self, X):
        """
        Fait une pr√©diction avec estimation de la confiance
        """
        results = []
        
        for x in X:
            x = x.reshape(1, -1)
            
            # 1. V√©rifier si l'entr√©e est trop diff√©rente des donn√©es d'entra√Ænement
            mahalanobis_dist = self._compute_mahalanobis_distance(x)
            
            # 2. Obtenir la pr√©diction et les probabilit√©s
            probs = self.mlp._forward_propagation(x.T)[0].ravel()
            prediction = np.argmax(probs)
            max_prob = np.max(probs)
            
            # 3. V√©rifier les crit√®res de confiance
            is_confident = (mahalanobis_dist <= self.mahalanobis_threshold and 
                          max_prob >= self.softmax_threshold)
            
            if is_confident:
                results.append({
                    'prediction': prediction,
                    'confidence': max_prob,
                    'status': 'confident'
                })
            else:
                results.append({
                    'prediction': None,
                    'confidence': None,
                    'status': 'unknown',
                    'reason': ('high_mahalanobis' if mahalanobis_dist > self.mahalanobis_threshold 
                             else 'low_confidence')
                })
                
        return results
    
    def evaluate_with_rejection(self, X, y):
        """
        √âvalue le mod√®le en permettant le rejet des pr√©dictions peu s√ªres
        """
        predictions = self.predict_with_confidence(X)
        
        # Statistiques
        total = len(predictions)
        rejected = sum(1 for p in predictions if p['status'] == 'unknown')
        confident = total - rejected
        
        # Calculer l'accuracy seulement sur les pr√©dictions confiantes
        confident_preds = [p['prediction'] for p in predictions if p['status'] == 'confident']
        confident_true = [y[i] for i, p in enumerate(predictions) if p['status'] == 'confident']
        
        if confident_preds:
            accuracy = sum(p == t for p, t in zip(confident_preds, confident_true)) / len(confident_preds)
        else:
            accuracy = 0
            
        return {
            'total_samples': total,
            'confident_predictions': confident,
            'rejected_predictions': rejected,
            'rejection_rate': rejected / total,
            'accuracy_on_confident': accuracy
        }

Cette impl√©mentation utilise plusieurs techniques pour d√©tecter les entr√©es "hors distribution" :

Distance de Mahalanobis :

Mesure la distance entre une nouvelle entr√©e et la distribution des donn√©es d'entra√Ænement
Permet de d√©tecter si une entr√©e est statistiquement diff√©rente des exemples connus

Confiance du softmax :

V√©rifie si la probabilit√© maximale de classification est suffisamment √©lev√©e
Une faible confiance peut indiquer une entr√©e ambigu√´

M√©canisme de rejet :

Retourne "unknown" si l'entr√©e ne satisfait pas les crit√®res de confiance
Fournit une raison pour le rejet (distance trop grande ou confiance trop faible)

Exemple d'utilisation :
pythonCopy# Cr√©er et entra√Æner le mod√®le
model = RobustMLP(input_size=784, hidden_sizes=[128, 64], output_size=10)
model.fit(X_train, y_train)

# Faire une pr√©diction avec possibilit√© de rejet
results = model.predict_with_confidence(X_test)

# Pour chaque pr√©diction
for result in results:
    if result['status'] == 'confident':
        print(f"Prediction: {result['prediction']}, Confidence: {result['confidence']:.2f}")
    else:
        print(f"Unknown input (Reason: {result['reason']})")

# √âvaluer avec m√©canisme de rejet
metrics = model.evaluate_with_rejection(X_test, y_test)
print(f"Rejection rate: {metrics['rejection_rate']:.2%}")
print(f"Accuracy on confident predictions: {metrics['accuracy_on_confident']:.2%}")
Cette approche offre plusieurs avantages :

Capacit√© √† dire "je ne sais pas" quand n√©cessaire
D√©tection robuste des entr√©es hors distribution
Mesures de confiance multiples
Explication des rejets
M√©triques d'√©valuation adapt√©es